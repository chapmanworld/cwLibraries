(*
  AUTHOR: Craig Chapman for ChapmanWorld LLC.
  PROPERTY OF: ChapmanWorld LLC.
  ALL RIGHTS RESERVED.
*)
unit utlLexers.json;

interface

const
  ///  <summary>
  ///    The name of the lexer is defined here for any unit which uses
  ///    this one to find. The consuming unit can instance a lexer from
  ///    utlLexers with the following line ... <br/> <br/>
  ///      Lexer := Lexers.CreateLexer( cJsonLexer, Filename );
  ///  </summary>
  cJsonLexer = 'jsonLexer';

type
  ///  <summary>
  ///    This is an enumeration of the tokens that will be generated by our
  ///    lexer. This is here for consuming units which implement parsers to
  ///    find, so that they can parse the stream of tokens produced by the
  ///    lexer.
  ///  </summary>
  TJsonTokens = (

    ///  <summary>
    ///     The token returned by the lexer when it is unable to determine
    ///     the next token in the input stream
    ///  </summer>
    tkUnknown,

    ///  <summary>
    ///    The token returned by the lexer when it reaches the end of the
    ///    input stream.
    ///  </summary>
    tkEOS,

    ///  <summary>
    ///    RFC 8259: '[' a left square bracket.
    ///  </summary>
    tkBeginArray,

    ///  <summary>
    ///    RFC 8259: ']' a right square bracket.
    ///  </summary>
    tkEndArray,

    ///  <summary>
    ///    RFC 8259: '{' a left brace
    ///  </summary>
    tkBeginObject,

    ///  <summary>
    ///    RFC 8259: '}' a right brace
    ///  </summary>
    tkEndObject,

    ///  <summary>
    ///    RFC 8259: ':' a colon
    ///  </summary>
    tkNameSeparator,

    ///  <summary>
    ///    RFC 8259: ',' a comma
    ///  </summary>
    tkValueSeparator,

    ///  <summary>
    ///    The reserved word 'false' <br/>
    ///    RFC 8259: The literal name MUST be lowercase. <br/>
    ///              false = %x66.61.6c.73.65   ; false
    ///  </summary>
    tkResFalse,

    ///  <summary>
    ///    The reserved word 'true' <br/>
    ///    RFC 8259: The literal name MUST be lowercase. <br/>
    ///              true  = %x74.72.75.65      ; true
    ///  </summary>
    tkResTrue,

    ///  <summary>
    ///    The reserved word 'null'  <br/>
    ///    RFC 8259: The literal name MUST be lowercase.  <br/>
    ///              null  = %x6e.75.6c.6c      ; null
    ///  </summary>
    tkResNull,

    ///  <summary>
    ///    Represents a string. <br/>
    ///    See RFC 8259 for token parsing rules for strings, as implemented
    ///    within the tokenizer.
    ///  </summary>
    tkString,

    ///  <summary>
    ///    RFC 8259 : <br/>
    ///      number = [ minus ] int [ frac ] [ exp ] <br/>
    ///      decimal-point = %x2E       ; . <br/>
    ///      digit1-9 = %x31-39         ; 1-9 <br/>
    ///      e = %x65 / %x45            ; e E <br/>
    ///      exp = e [ minus / plus ] 1*DIGIT <br/>
    ///      frac = decimal-point 1*DIGIT <br/>
    ///  </summary>
    tkNumber

  );

implementation
uses
  SysUtils
, utlTypes
, utlLexers
;

const
  DefaultWhitespace = [ ' ', TAB, CR, LF ];

type
  ///  <summary>
  ///    Here we write our tokenizer class, which we provide to be injected
  ///    into our lexer in order to provide the critical functionality.
  ///  </summary>
  TTokenizer = class( TInterfacedObject, ITokenizer< TJsonTokens > )
  private
    procedure TokenizeResFalse (const Scanner: IScanner; var Token: TJsonTokens; var Data: string );
    procedure TokenizeResNull( const Scanner: IScanner; var Token: TJsonTokens; var Data: string );
    procedure TokenizeResTrue( const Scanner: IScanner; var Token: TJsonTokens; var Data: string );
    procedure TokenizeString( const Scanner: IScanner; var Token: TJsonTokens; var Data: string );
    procedure TokenizeNumber( const Scanner: IScanner; var Token: TJsonTokens; var Data: string );
  strict private //- ITokenizer< TJsonTokens > -//

    ///  <summary>
    ///    Simply return tkEOS, our token for end of stream.
    ///  </summary>
    function EOSToken: TJsonTokens;

    ///  <summary>
    ///    Simply return tkUnknown, our token when we can't determine the next token
    ///  </summary>
    function UnknownToken: TJsonTokens;

    ///  <summary>
    ///    Skips spaces, carriage returns, line feeds, and tabs when they are
    ///    found on the scanner. See implementation.
    ///  </summary>
    procedure SkipWhitespace( const Scanner: IScanner );

    ///  <summary>
    ///    The tokenizer method which does the actual work. <br/>
    ///    Attempts to identify the next token based on the scanner, and
    ///    sets the Token and Data var parameters accordingly. <br/>
    ///    If a token cannot be identified, this method should not alter the
    ///    token parameter, it defaults to our unknown token when provided
    ///    by the lexer.
    ///  </summary>
    procedure GetNextToken( const Scanner: IScanner; var Token: TJsonTokens; var Data: string );
  public
    ///  <summary>
    ///    The factory (required) does nothing but return a new instance of our
    ///    tokenizer.
    ///  </summary>
    class function Factory: ITokenizer< TJsonTokens >; static;

    ///  <summary>
    ///    The equality (required) compares the tokens A and B, returning
    ///    true if they are equal, else returning false.
    ///  </summary>
    class function Equality( const A, B: TJsonTokens ): boolean; static;
  end;

function TTokenizer.EOSToken: TJsonTokens;
begin
  Result := tkEOS;
end;

class function TTokenizer.Equality( const A, B: TJsonTokens ): boolean;
begin
  Result := A = B;
end;

class function TTokenizer.Factory: ITokenizer< TJsonTokens >;
begin
  Result := TTokenizer.Create;
end;

procedure TTokenizer.TokenizeResTrue( const Scanner: IScanner; var Token: TJsonTokens; var Data: string );
begin
  // For a more complex language, we would gather the characters until
  // the end of the token or the end of the stream, and compare the collected
  // characters to a reserved word string. Json however has only three reserved
  // words and they are all short, so performing character-by-character
  // identifiation will be faster and simpler.
  // We enter here with 't' already identified.
  Scanner.Advance;
  if Scanner.EndOfStream then exit;
  if Scanner.Current <> 'r' then exit;
  Scanner.Advance;
  if Scanner.EndOfStream then exit;
  if Scanner.Current <> 'u' then exit;
  Scanner.Advance;
  if Scanner.EndOfStream then exit;
  if Scanner.Current <> 'e' then exit;
  Token := tkResTrue;
  // we don't advance the scanner, because GetNextToken() will do it.
end;

procedure TTokenizer.TokenizeResFalse( const Scanner: IScanner; var Token: TJsonTokens; var Data: string );
begin
  // For a more complex language, we would gather the characters until
  // the end of the token or the end of the stream, and compare the collected
  // characters to a reserved word string. Json however has only three reserved
  // words and they are all short, so performing character-by-character
  // identifiation will be faster and simpler.
  // We enter here with 'f' already identified.
  Scanner.Advance;
  if Scanner.EndOfStream then exit;
  if Scanner.Current <> 'a' then exit;
  Scanner.Advance;
  if Scanner.EndOfStream then exit;
  if Scanner.Current <> 'l' then exit;
  Scanner.Advance;
  if Scanner.EndOfStream then exit;
  if Scanner.Current <> 's' then exit;
  Scanner.Advance;
  if Scanner.EndOfStream then exit;
  if Scanner.Current <> 'e' then exit;
  Token := tkResFalse;
  // we don't advance the scanner, because GetNextToken() will do it.
end;

procedure TTokenizer.TokenizeResNull( const Scanner: IScanner; var Token: TJsonTokens; var Data: string );
begin
  // For a more complex language, we would gather the characters until
  // the end of the token or the end of the stream, and compare the collected
  // characters to a reserved word string. Json however has only three reserved
  // words and they are all short, so performing character-by-character
  // identifiation will be faster and simpler.
  // We enter here with 'n' already identified.
  Scanner.Advance;
  if Scanner.EndOfStream then exit;
  if Scanner.Current <> 'u' then exit;
  Scanner.Advance;
  if Scanner.EndOfStream then exit;
  if Scanner.Current <> 'l' then exit;
  Scanner.Advance;
  if Scanner.EndOfStream then exit;
  if Scanner.Current <> 'l' then exit;
  Token := tkResTrue;
  // we don't advance the scanner, because GetNextToken() will do it.
end;

procedure TTokenizer.TokenizeString( const Scanner: IScanner; var Token: TJsonTokens; var Data: string );
begin
  // We enter with '"' already identified.
  Scanner.Advance;
  if Scanner.EndOfStream then exit;
  repeat
    if Scanner.EndOfStream then exit;

    // Watch for escaped characters
    if Scanner.Current = '\' then begin
      Scanner.Advance;
      if Scanner.EndOfStream then exit;
      //- Currently we simply escape any character, [TODO] add unicode sequences
      Data := Data + Scanner.Current;
      Scanner.Advance;
      continue;
    end;

    // If the character is a '"' we're done
    // Remember not to remove the quote, GetNextToken() will do that.
    if Scanner.Current = '"' then begin
      Token := tkString;
      exit;
    end;

    // Any other character gets added to Data
    Data := Data + Scanner.Current;
    Scanner.Advance;
  until False;
end;

procedure TTokenizer.TokenizeNumber( const Scanner: IScanner; var Token: TJsonTokens; var Data: string );
begin
  // Optional sign
  if CharInSet( Scanner.Current, [ '-', '+' ] ) then begin
    Data := Data + Scanner.Current;
    Scanner.Advance;
  end;
  if Scanner.EndOfStream then exit;
  // Expect integer part.
  if Scanner.Current = '0' then begin
    Data := Data + Scanner.Current;
    Scanner.Advance;
  end else if CharInSet( Scanner.Current, [ '0'..'9' ] ) then begin
    while CharInSet( Scanner.Current, [ '0'..'9' ] ) do begin
      Data := Data + Scanner.Current;
      Scanner.Advance;
      if Scanner.EndOfStream then exit;
    end;
  end else exit;
  // Optional fractional part.
  if Scanner.Current <> '.' then begin
    Token := tkNumber;
    exit;
  end;
  Data := Data + Scanner.Current;
  Scanner.Advance; // beyond period
  if Scanner.EndOfStream then exit;
    while CharInSet( Scanner.Current, [ '0'..'9' ] ) do begin
      Data := Data + Scanner.Current;
      Scanner.Advance;
      if Scanner.EndOfStream then exit;
    end;
  // Optional exponenet part.
  if not CharInSet( Scanner.Current, [ 'e', 'E' ] ) then begin
    Token := tkNumber;
    exit;
  end;
  Data := Data + Scanner.Current;
  Scanner.Advance; // beyond 'e'/'E'
  if Scanner.EndOfStream then exit;
  Token := tkNumber;
  while CharInSet( Scanner.Current, [ '0'..'9' ] ) do begin
    Data := Data + Scanner.Current;
    Scanner.Advance;
    if Scanner.EndOfStream then exit;
  end;
end;

procedure TTokenizer.GetNextToken( const Scanner: IScanner; var Token: TJsonTokens; var Data: string );
begin
  case Scanner.Current of
    '[': Token := tkBeginArray;
    ']': Token := tkEndArray;
    '{': Token := tkBeginObject;
    '}': Token := tkEndObject;
    ',': Token := tkValueSeparator;
    ':': Token := tkNameSeparator;
    't': TokenizeResTrue( Scanner, Token, Data );
    'f': TokenizeResFalse( Scanner, Token, Data );
    'n': TokenizeResNull( Scanner, Token, Data );
    '"': TokenizeString( Scanner, Token, Data );
    else begin
      if CharInSet( Scanner.Current, [ '-', '+', '0' .. '9', '.' ] ) then TokenizeNumber( Scanner, Token, Data );
    end;
  end;
  if Token <> tkUnknown then Scanner.Advance;
end;

procedure TTokenizer.SkipWhitespace( const Scanner: IScanner );
begin
  (* RFC 8259:
         ws = *(
              %x20 /              ; Space
              %x09 /              ; Horizontal tab
              %x0A /              ; Line feed or New line
              %x0D )              ; Carriage return
  *)
  while ( not Scanner.EndOfStream ) and ( CharInSet( Scanner.Current, DefaultWhitespace ) ) do Scanner.Advance;
end;

function TTokenizer.UnknownToken: TJsonTokens;
begin
  Result := tkUnknown;
end;

initialization
  Lexers.RegisterLexer< TJsonTokens >( cJsonLexer, TTokenizer.Factory, TTokenizer.Equality );

end.
